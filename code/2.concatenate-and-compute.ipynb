{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba449e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deb3a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(directory):\n",
    "    '''\n",
    "    Reads all JSON files in a given directory into a single pandas dataframe.\n",
    "    (Disclaimer: this function written by ChatGPT and edited by me)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    directory : str\n",
    "        The path to the directory containing the JSON files.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        A single dataframe containing all data from the JSON files.\n",
    "    '''\n",
    "   \n",
    "    # Initialize an empty list to store the dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # Loop through all the JSON files in the directory\n",
    "    for filename in glob.glob(directory + '/*.json'):\n",
    "\n",
    "        # Read the JSON file into a pandas dataframe\n",
    "        with open(filename, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            df = pd.DataFrame(json_data)\n",
    "            dfs.append(df)\n",
    "\n",
    "    # Concatenate all the dataframes into a single dataframe\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce08ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_date_interval(df):\n",
    "    \"\"\"\n",
    "    Adds four columns to a pandas DataFrame that contain the date 7 days before and\n",
    "    after each date in the DataFrame, the original date in YYYY-MM-DD format, and a week ID.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to which the date interval columns will be added. \n",
    "        This DataFrame must contain a 'date' column that contains datetime objects.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The input DataFrame with four new columns: '7_days_before', \n",
    "        '7_days_after', 'date_str', and 'week_id'.\n",
    "\n",
    "        These columns contain the date 7 days before and after each date in the 'date' column of the input DataFrame,\n",
    "        the date in YYYY-MM-DD format, and a week identifier in the format 'YEAR_WEEK'.\n",
    "\n",
    "    Disclaimer:\n",
    "        This docstring was written by ChatGPT, a large language model trained by OpenAI.\n",
    "    \"\"\"\n",
    "    df['datetime'] = pd.to_datetime(df.date)\n",
    "    \n",
    "    df['date_str'] = df.datetime.dt.strftime(\"%Y-%m-%d\")\n",
    "    df['7_days_before'] = df['datetime'] - pd.DateOffset(days=7)\n",
    "    df['7_days_after'] = df['datetime'] + pd.DateOffset(days=7)\n",
    "    df['week_id'] = df.datetime.dt.year.astype(str) + \"_\" + df.datetime.dt.isocalendar().week.astype(str)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d0bafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_group_statistics(df: pd.DataFrame, group: list, columns: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each item of interest, compute the average and standard deviation from each group.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        group (list): A list containing the columns over which a groupby will be applied.\n",
    "        columns (list): A list of column names to compute the statistics.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame with the average and standard deviation for each group.\n",
    "\n",
    "    Disclaimer:\n",
    "        This docstring was written by ChatGPT, a large language model trained by OpenAI.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the average and standard deviation from each group\n",
    "    avgs = df.groupby(group)[columns].mean().reset_index()\n",
    "    stdevs = df.groupby(group)[columns].std().reset_index()\n",
    "    percentiles = df.groupby(group)[columns].quantile(.9).reset_index()\n",
    "    \n",
    "    # Rename the columns to avoid naming conflicts\n",
    "    avgs = avgs.rename(columns = {\n",
    "        col_name: f'group_avg_for_{col_name}' for col_name in avgs.columns if col_name not in ['user', 'date_str', 'week_id']\n",
    "    })\n",
    "\n",
    "    stdevs = stdevs.rename(columns = {\n",
    "        col_name: f'group_stdev_for_{col_name}'for col_name in stdevs.columns if col_name not in ['user', 'date_str', 'week_id']\n",
    "    })\n",
    "    \n",
    "    percentiles = percentiles.rename(columns = {\n",
    "        col_name: f'group_90th_percentile_for_{col_name}'for col_name in percentiles.columns if col_name not in ['user', 'date_str', 'week_id']\n",
    "    })    \n",
    "\n",
    "    # Merge the two DataFrames and return the result\n",
    "    result = pd.merge(avgs, stdevs, on=group, suffixes=[\"\",\"_y\"]).merge(percentiles, on=group, suffixes=[\"\", \"_y\"])\n",
    "    result = result.drop(columns=[col for col in result.columns if \"_y\" in col])\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c25984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diffs(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each item, computes how many standard deviations we are away from the average.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The pandas DataFrame containing the data.\n",
    "    - column (str): The name of the column to compute differences for.\n",
    "\n",
    "    Returns:\n",
    "    - The input DataFrame with a new column added, containing the computed differences.\n",
    "\n",
    "    Example:\n",
    "    >>> data = pd.DataFrame({'values': [1, 2, 3, 4, 5],\n",
    "                             'group_avg_for_values': [3, 3, 3, 3, 3],\n",
    "                             'group_stdev_for_values': [1, 1, 1, 1, 1]})\n",
    "    >>> result = compute_diffs(data, 'values')\n",
    "    >>> print(result)\n",
    "       values  group_avg_for_values  group_stdev_for_values  values_diff\n",
    "    0       1                     3                       1         -2.0\n",
    "    1       2                     3                       1         -1.0\n",
    "    2       3                     3                       1          0.0\n",
    "    3       4                     3                       1          1.0\n",
    "    4       5                     3                       1          2.0\n",
    "    \n",
    "    Disclaimer:\n",
    "        This docstring was written by ChatGPT, a large language model trained by OpenAI.\n",
    "    \"\"\"\n",
    "    \n",
    "    df[f'{column}_diff'] = (df[column] - df[f'group_avg_for_{column}']) / df[f'group_stdev_for_{column}']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab3ecda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_quantiles(df: pd.DataFrame, group: list, column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each item, compute the quantile relative to all other items in the same group.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): The pandas DataFrame containing the data.\n",
    "    - group (list): A list containing the columns over which a groupby will be applied.\n",
    "    - column (str): The name of the column to compute differences for.\n",
    "\n",
    "    Returns:\n",
    "    - The input DataFrame with a new column added, containing the computed quantiles\n",
    "    \n",
    "    Disclaimer:\n",
    "        This docstring was written by ChatGPT, a large language model trained by OpenAI.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Adapted from https://stackoverflow.com/questions/37093088/pandas-create-percentile-field-based-on-groupby-with-level-1/37093160#37093160\n",
    "    df[f'percentile_for_{column}'] = df.groupby(group) \\\n",
    "        .total_engagement \\\n",
    "        .apply(lambda series: \\\n",
    "                pd.Series(\n",
    "                    [stats.percentileofscore(series, row, kind='rank') for row in series], # See https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.percentileofscore.html\n",
    "                    index=series.index\n",
    "                )\n",
    "              )\n",
    "    \n",
    "    df[f'percentile_for_{column}'] = 100 - df[f'percentile_for_{column}']\n",
    "    df[f'percentile_for_{column}'] = df[f'percentile_for_{column}'].round()\n",
    "               \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f85295f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Reads the collected tweets\n",
    "    df = read_files(\"../../output/mvp/1.raw_tweets/jsons/\")\n",
    "    \n",
    "    # Creates date columns\n",
    "    df = set_date_interval(df)\n",
    "    \n",
    "    # Creates measures of interest: total engagement, engagement with no interaction,\n",
    "    # engagement with interaction, interaction ratios\n",
    "    df['total_engagement'] = df.like_count + df.retweet_count + df.quote_count + df.reply_count\n",
    "#     df['non_interactive_engagement'] = df.like_count + df.retweet_count\n",
    "#     df['interactive_engagement'] = df.quote_count + df.reply_count\n",
    "#     df['interactive_ratio'] = df.interactive_engagement / df.total_engagement\n",
    "    \n",
    "#     # Computes the averages and standard deviations for the group\n",
    "#     gpby = compute_group_statistics(df, ['user_id', 'week_id'], [\"like_count\", \"retweet_count\", \"quote_count\", \"reply_count\",\n",
    "#     \"total_engagement\", \"non_interactive_engagement\", \"interactive_engagement\",\"interactive_ratio\"])\n",
    "    \n",
    "#     gpby = compute_group_statistics(df, ['user', 'week_id'], [\"total_engagement\"])\n",
    "    \n",
    "    # Joins the computed values with the entire data frame\n",
    "#     df = df.merge(gpby, on=['user', 'date_str'])\n",
    "#     df = df.merge(gpby, on=['user', 'week_id'])    \n",
    "    df = compute_quantiles(df, ['user', 'week_id'], \"total_engagement\")\n",
    "    \n",
    "    # Computes the differences\n",
    "#     for column in [\"like_count\", \"retweet_count\", \"quote_count\", \"reply_count\",\n",
    "#         'total_engagement', 'non_interactive_engagement', 'interactive_engagement', 'interactive_ratio']:\n",
    "#         df = compute_diffs(df, column)\n",
    "        \n",
    "    df.to_csv(\"../../output/mvp/2.tweets_with_relative_engagement/concatenated-tweets.csv\", index=False)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8fb026b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/52/vy6xx2q911l7rd368b7ngb8w0000gn/T/ipykernel_12755/1956186762.py:17: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda series: \\\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
